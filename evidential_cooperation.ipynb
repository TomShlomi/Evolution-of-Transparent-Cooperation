{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance metrics\n",
    "# Defines a distance metric between two neural networks which is the mean squared difference between the weights of the two networks times a constant\n",
    "def distance_mse(network1, network2):\n",
    "    distance = 0\n",
    "    total_params = 0\n",
    "    for param1, param2 in zip(network1.parameters(), network2.parameters()):\n",
    "        distance += torch.mean((param1 - param2) ** 2)*param1.numel()\n",
    "        total_params += param1.numel()\n",
    "    return distance/total_params/0.07\n",
    "\n",
    "# Defines a null distance metric which always returns 0\n",
    "def distance_null(network1, network2):\n",
    "    return 0\n",
    "\n",
    "# Defines a discrete distance metric which is 0 if the two networks are equal and 1 otherwise\n",
    "def distance_discrete(network1, network2):\n",
    "    for param1, param2 in zip(network1.parameters(), network2.parameters()):\n",
    "        if not torch.equal(param1, param2):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Games\n",
    "# Returns the game matrix of various dilemmas dilemma in tensor form. Shifts the rewards to [-1, 0] to aid learning\n",
    "def get_prisoners_dilemma():\n",
    "    return torch.tensor([[[3, 0], [5, 1]], [[3, 5], [0, 1]]], dtype=torch.float)/6 - 1\n",
    "\n",
    "def get_stag_hunt():\n",
    "    return torch.tensor([[[2, 0], [1, 1]], [[2, 1], [0, 1]]], dtype=torch.float)/3 - 1\n",
    "\n",
    "def get_uniform_game():\n",
    "    return torch.rand(2, 2, 2, dtype=torch.float) - 1\n",
    "\n",
    "def get_normal_game():\n",
    "    return torch.randn(2, 2, 2, dtype=torch.float) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent class\n",
    "class Agent:\n",
    "    # Initialize the agent with a neural network and a distance metric\n",
    "    def __init__(self, network, distance_f, criterion=nn.MSELoss(), optimizer=torch.optim.SGD, lr=0.001):\n",
    "        self.network = network\n",
    "        self.score = 0\n",
    "        self.distance_f = distance_f\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer(self.network.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment class\n",
    "# Create an environment in which RL agents interact through one-shot games and occaisionally reproduce\n",
    "class Environment:\n",
    "    # Initialize the environment with a list of agents and a matrix game generator\n",
    "    def __init__(self, agents, game_generator):\n",
    "        self.agents = agents\n",
    "        self.game_generator = game_generator\n",
    "    \n",
    "    # Play a one-shot game between two agents\n",
    "    def play_game(self, game, agent1, agent2):\n",
    "        game_flattened = game.flatten()\n",
    "        # Get the distance from the agents to each other (note that the distance is not necessarily symmetric)\n",
    "        distance1 = agent1.distance_f(agent1.network, agent2.network)\n",
    "        distance2 = agent2.distance_f(agent2.network, agent1.network)\n",
    "        # Combine the game and the distances into a single tensor for each agent\n",
    "        input1 = torch.cat((game_flattened, torch.tensor([distance1])))\n",
    "        input2 = torch.cat((game_flattened, torch.tensor([distance2])))\n",
    "        # Get the output of each agent's network\n",
    "        output1 = agent1.network(input1)\n",
    "        output2 = agent2.network(input2)\n",
    "        # Get the action of each agent\n",
    "        action1 = torch.argmax(output1)\n",
    "        action2 = torch.argmax(output2)\n",
    "        # Get the reward of each agent\n",
    "        reward1 = game[0, action1, action2]\n",
    "        reward2 = game[1, action1, action2]\n",
    "        # Update the score of each agent\n",
    "        agent1.score += reward1\n",
    "        agent2.score += reward2\n",
    "        # Zero the gradients of each agent's network\n",
    "        agent1.network.zero_grad()\n",
    "        agent2.network.zero_grad()\n",
    "        # Calculate the loss of each agent's network\n",
    "        loss1 = agent1.criterion(output1[action1], reward1)\n",
    "        loss2 = agent2.criterion(output2[action2], reward2)\n",
    "        # Backpropagate the loss of each agent's network\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        # Update the parameters of each agent's network\n",
    "        agent1.optimizer.step()\n",
    "        agent2.optimizer.step()\n",
    "\n",
    "    # Has all of the agents play one-shot games a certain number of times, then updates the set of agents\n",
    "    def round(self, num_games, death_rate, print_average_score=False):\n",
    "        for _ in range(num_games):\n",
    "            random.shuffle(self.agents)\n",
    "            # Iterate through every other agent\n",
    "            for i in range(0, len(self.agents), 2):\n",
    "                # Play a game between the current agent and the next agent\n",
    "                self.play_game(self.game_generator(), self.agents[i], self.agents[i+1])\n",
    "        # Sort the agents by score\n",
    "        self.agents.sort(key=lambda agent: agent.score, reverse=True)\n",
    "        # Print the average score(?) and zero out the scores\n",
    "        if print_average_score:\n",
    "            scores = [agent.score/num_games for agent in self.agents]\n",
    "            print((sum(scores) / len(scores)).item())\n",
    "        for agent in self.agents:\n",
    "            agent.score = 0\n",
    "        death_count = int(len(self.agents) * death_rate)\n",
    "        for i in range(death_count):\n",
    "            self.agents[-i-1] = Agent(copy.deepcopy(self.agents[i].network), self.agents[i].distance_f)\n",
    "\n",
    "    # Run the environment for a certain number of rounds\n",
    "    def run(self, num_rounds, num_games, death_rate, print_average_score=True):\n",
    "        for i in range(num_rounds):\n",
    "            print(f'Round {i+1}:', end=' ')\n",
    "            self.round(num_games, death_rate, print_average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
